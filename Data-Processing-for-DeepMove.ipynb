{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b12072",
   "metadata": {},
   "source": [
    "# Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e78992ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_neural = \\\n",
    "{'train_loc': {1: 2, 2: 1, 3: 1, 4: 12, 5: 1, 6: 2, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 2, 25: 2, 26: 1, 27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1}, \n",
    "\n",
    "'explore': 0.15384615384615385, \n",
    "\n",
    "'train': [0, 1, 2, 3, 4], \n",
    "\n",
    "'entropy': 3.219290807235925, \n",
    "\n",
    "'rg': 0.03142983221874803, \n",
    "\n",
    "'sessions': {\n",
    "0: [[1, 30], [2, 45], [3, 46], [4, 14], [4, 16], [5, 20], [6, 23], [7, 0], [4, 13], [8, 20], [9, 21]], \n",
    "1: [[10, 40], [4, 14], [11, 14], [12, 22], [13, 20], [4, 24], [14, 43], [4, 14], [15, 17], [4, 18], [16, 21]], \n",
    "2: [[17, 22], [4, 14], [18, 18], [4, 18], [19, 24], [20, 27], [21, 27]], \n",
    "3: [[22, 19], [4, 17], [23, 25], [24, 40], [25, 0], [4, 19], [4, 14], [26, 22], [6, 23]], \n",
    "4: [[27, 24], [28, 25], [1, 26], [29, 46], [25, 24], [24, 40], [30, 43], [31, 44], [32, 46], [33, 2]], \n",
    "5: [[1, 27], [4, 17], [34, 21], [35, 22], [4, 13], [36, 2], [4, 13], [37, 19], [38, 20], [36, 2], [36, 26]], 6: [[39, 45], [10, 47], [35, 45], [36, 3], [4, 13]]\n",
    "},\n",
    "\n",
    "'test': [5, 6],\n",
    "\n",
    "'valid_len': 14, \n",
    "\n",
    "'pred_len': 43}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681ea2b4",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fc51c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# # 1k data\n",
    "df_train = pd.read_csv('df_train_1k.csv')\n",
    "df_test  = pd.read_csv('df_test_1k.csv')\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]\n",
    "\n",
    "# Sliding window by 1 STEP\n",
    "STEP_SIZE = 12\n",
    "WINDOW_SIZE = 10\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = []\n",
    "        for group in grouped_data:\n",
    "            if len(group) >= STEP_SIZE:\n",
    "                # get the first STEP_SIZE location and time data\n",
    "                xy = group['combined_xy'].values.tolist()[:STEP_SIZE]\n",
    "                t = group['t'].values.tolist()[:STEP_SIZE]\n",
    "                # slice the data into several sessions using moving window approach\n",
    "                self.data.extend([(xy[i:i+WINDOW_SIZE], t[i:i+WINDOW_SIZE])\n",
    "                                  for i in range(STEP_SIZE-WINDOW_SIZE+1)])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        xy_window, t_window = self.data[idx]\n",
    "        inputs = torch.tensor(xy_window[:-1])        # input sequence of locations\n",
    "        labels = torch.tensor(xy_window[-1])         # desired predicted location\n",
    "        positions = torch.tensor(t_window[:-1])      # corresponding input locations' times\n",
    "        label_positions = torch.tensor(t_window[-1]) # corresponding predicted location's time\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "# Sliding window by STEP_SIZE\n",
    "STEP_SIZE = 20\n",
    "WINDOW_SIZE = 10\n",
    "class TrajectoryDatasetTEST(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = []\n",
    "        for group in grouped_data:\n",
    "            if len(group) >= STEP_SIZE:\n",
    "                # get the first STEP_SIZE location and time data\n",
    "                xy = group['combined_xy'].values.tolist()[:STEP_SIZE]\n",
    "                t = group['t'].values.tolist()[:STEP_SIZE]\n",
    "                # slice the data into several sessions using moving window approach\n",
    "                self.data.extend([(xy[i:i+WINDOW_SIZE], t[i:i+WINDOW_SIZE])\n",
    "                                  for i in range(0, STEP_SIZE-WINDOW_SIZE+1, STEP_SIZE)])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        xy_window, t_window = self.data[idx]\n",
    "        inputs = torch.tensor(xy_window[:-1])        # input sequence of locations\n",
    "        labels = torch.tensor(xy_window[-1])         # desired predicted location\n",
    "        positions = torch.tensor(t_window[:-1])      # corresponding input locations' times\n",
    "        label_positions = torch.tensor(t_window[-1]) # corresponding predicted location's time\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train)\n",
    "test_dataset  = TrajectoryDatasetTEST(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32876706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataneural(train_dataset, test_dataset):\n",
    "    train_keys = [0,1,2,3]\n",
    "    test_keys  = [4,5]\n",
    "    \n",
    "    data_neural = {}\n",
    "    data_neural_index = 0\n",
    "    \n",
    "    test_data = test_dataset.data\n",
    "    index = 0\n",
    "    for i in range(0, len(test_data), 2):\n",
    "        data_neural[index]={'sessions':{}}\n",
    "        data_neural[index]['sessions'][0]=[[location,time] for location,time in zip(test_data[i][0],test_data[i][1])]\n",
    "        data_neural[index]['sessions'][1]=[[location,time] for location,time in zip(test_data[i+1][0],test_data[i+1][1])]\n",
    "        data_neural[index]['train']=train_keys\n",
    "        data_neural[index]['test']=test_keys\n",
    "        index += 1\n",
    "    \n",
    "    train_data = train_dataset.data\n",
    "    index = 0\n",
    "    for i in range(0, len(train_data), 4):\n",
    "        data_neural[index]['sessions'][2]=[[location,time] for location,time in zip(train_data[i][0],train_data[i][1])]\n",
    "        data_neural[index]['sessions'][3]=[[location,time] for location,time in zip(train_data[i+1][0],train_data[i+1][1])]\n",
    "        data_neural[index]['sessions'][4]=[[location,time] for location,time in zip(train_data[i+2][0],train_data[i+2][1])]\n",
    "        data_neural[index]['sessions'][5]=[[location,time] for location,time in zip(train_data[i+3][0],train_data[i+3][1])]\n",
    "        index += 1\n",
    "        if (index == len(data_neural)):\n",
    "            break\n",
    "        \n",
    "    return data_neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcc012cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['data_neural'] = format_dataneural(train_dataset, test_dataset)\n",
    "uid_list = [int(i) for i in range(len(output['data_neural']))]\n",
    "output['uid_list'] = uid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5300682",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('DeepMove-master/data/my_data_neural.pk', 'wb') as f:\n",
    "    pickle.dump(output, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
