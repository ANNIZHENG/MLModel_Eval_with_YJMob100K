{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb661d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e5381",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9d9d693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv')\n",
    "df_test = pd.read_csv('df_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069914f",
   "metadata": {},
   "source": [
    "# Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55a845b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]\n",
    "\n",
    "STEP_SIZE = 100\n",
    "WINDOW_SIZE = 10\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = []\n",
    "        for group in grouped_data:\n",
    "            if len(group) >= STEP_SIZE:\n",
    "                # get the first STEP_SIZE location and time data\n",
    "                xy = group['combined_xy'].values.tolist()[:STEP_SIZE]\n",
    "                t = group['t'].values.tolist()[:STEP_SIZE]\n",
    "                # slice the data into several sessions using moving window approach\n",
    "                self.data.extend([(xy[i:i+WINDOW_SIZE], t[i:i+WINDOW_SIZE])\n",
    "                                  for i in range(STEP_SIZE - WINDOW_SIZE + 1)])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        xy_window, t_window = self.data[idx]\n",
    "        inputs = torch.tensor(xy_window[:-1]) # input sequence of locations\n",
    "        labels = torch.tensor(xy_window[-1]) # desired predicted location\n",
    "        positions = torch.tensor(t_window[:-1]) # corresponding input locations' times\n",
    "        label_positions = torch.tensor(t_window[-1]) # corresponding predicted location's time\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train) \n",
    "test_dataset = TrajectoryDataset(grouped_data_test)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True) \n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    # positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0, batch_first=True) \n",
    "    # label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    return inputs_padded, labels_padded # positions_padded, label_positions_padded\n",
    "\n",
    "BATCH_SIZE = (len(train_dataset) // len(grouped_data_train)) * 10\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be685e",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e39045",
   "metadata": {},
   "source": [
    "### Model Building (nn.PyTorch built-in LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "395293d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, loc_size, embed_dim, hidden_size, num_layers):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, loc_size)\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # positioanl embedding\n",
    "        x = self.input_embedding(x)\n",
    "        \n",
    "        # initiation\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  \n",
    "        \n",
    "        # getting output\n",
    "        out, _ = self.lstm(x, (h0, c0)) \n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5100074",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01df9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model related param\n",
    "\n",
    "model = LSTMModel(loc_size=40000, \n",
    "                  embed_dim=256, \n",
    "                  hidden_size=256, # not sure about this\n",
    "                  num_layers=2)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0013)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ed55f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        # inputs = inputs.float().unsqueeze(-1)\n",
    "        labels = labels.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    print(f\"Epoch: {epoch}: Average Loss: {total_loss/len(train_dataloader)}\")\n",
    "    \n",
    "\"\"\"\n",
    "Epoch: 0: Average Loss: 7.053878789760357\n",
    "Epoch: 1: Average Loss: 4.725554961066872\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797af25e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d27ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a6ffb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_top_k(model, dataloader, device, k):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            \n",
    "            if (BATCH_SIZE > 1):\n",
    "                for logit_i in range(len(logits)):\n",
    "                    tolerated_acc = int(labels[logit_i] in torch.topk(logits[logit_i], k)[1])\n",
    "                    total_correct += tolerated_acc\n",
    "            else:\n",
    "                tolerated_acc = int(labels in torch.topk(logits, k)[1])\n",
    "                total_correct += tolerated_acc\n",
    "\n",
    "            total_examples += labels.numel()\n",
    "    \n",
    "    print(f\"Batch Size: {BATCH_SIZE} Top {k} Accuracy: {total_correct/total_examples}, Num Correct: {total_correct}, Num Sample: {total_examples}\")\n",
    "    \n",
    "    return total_correct/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa01afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_accuracy = evaluate_model_with_top_k(model, test_dataloader, device, 5)\n",
    "\n",
    "\"\"\"\n",
    "Batch Size: 910 Top 5 Accuracy: 0.30905343392704365, Num Correct: 55629, Num Sample: 179998\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b35ac2",
   "metadata": {},
   "source": [
    "# Hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1146cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "\n",
    "batch_sizes = [int(i) for i in range(5,101,5)]\n",
    "batch_sizes.append(1)\n",
    "batch_sizes = sorted(batch_sizes)\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Set up dataloader\n",
    "    batch_size = trial.suggest_categorical('batch_size', batch_sizes)  ##\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model Parameters\n",
    "    NUM_CLASS = 40000\n",
    "    STEP_SIZE = 100\n",
    "    # EMBED_DIM = trial.suggest_categorical('embed_dim', [64, 128, 256, 512])\n",
    "    EMBED_DIM = 256\n",
    "    LAYER_DIM = trial.suggest_int('layer_dim', 1, 6)  ##\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Train the model\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    \n",
    "    # Model instantiation\n",
    "    model = LSTMModel(input_dim=1,\n",
    "                      embed_dim=EMBED_DIM, \n",
    "                      layer_dim=LAYER_DIM,\n",
    "                      output_dim=NUM_CLASS)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.float().unsqueeze(-1)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    final_avg_loss = total_loss / total_samples\n",
    "    return final_avg_loss\n",
    "\n",
    "# Hyperparameter tuning\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Result\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best loss:', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
