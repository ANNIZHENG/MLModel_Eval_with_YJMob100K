{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb661d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0fb9f",
   "metadata": {},
   "source": [
    "# Data-Processing \n",
    "(same process as in Transformer's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69a0f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_16785/4104450235.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "yjmob1 = 'yjmob100k-dataset1.csv.gz' # dataset under normal scenes\n",
    "yjmob_df = pd.read_csv(yjmob1, compression='gzip').sort_values(by=['uid', 'd', 't'], ignore_index=True)\n",
    "\n",
    "# Retrieve all ids\n",
    "uids = yjmob_df['uid'].unique()\n",
    "\n",
    "# Just to reduce memory space\n",
    "rand_indicies = [random.randint(0, len(uids)) for _ in range(200)] # only 200 data would be used\n",
    "selected_uids = [uid for uid in uids[rand_indicies]] # selected_uids = uids[:200]\n",
    "# selected_uids = uids[:200]\n",
    "\n",
    "df = yjmob_df[yjmob_df['uid'].isin(selected_uids)] \n",
    "\n",
    "# Time\n",
    "# df['combined_t'] = df['d']*47+df['t']\n",
    "\n",
    "# Location\n",
    "def spatial_token(x, y):\n",
    "    return (x-1)+(y-1)*200\n",
    "df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n",
    "\n",
    "# Sort value\n",
    "df = df.sort_values(by=['uid', 't'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e5381",
   "metadata": {},
   "source": [
    "# Train-Test Split \n",
    "(~Transformer's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f59ce97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:3 split\n",
    "train_uids, test_uids = train_test_split(selected_uids, test_size=0.30, random_state=42)\n",
    "\n",
    "# Load training and testing data\n",
    "df_train = df[df['uid'].isin(train_uids)]\n",
    "df_test = df[df['uid'].isin(test_uids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069914f",
   "metadata": {},
   "source": [
    "# Batching \n",
    "(~Transformer's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eb2545d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 55\n",
    "STEP_SIZE = 600 ## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f722893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(data, data_t):\n",
    "    return torch.tensor(data[:STEP_SIZE]),torch.tensor(data[STEP_SIZE]),\\\n",
    "                torch.tensor(data_t[:STEP_SIZE]),torch.tensor(data_t[STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1c25767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by uid\n",
    "grouped_data_train = df_train[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "grouped_data_test = df_test[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "03b87b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = grouped_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_for_uid = self.data[idx]\n",
    "        inputs, labels, positions, label_positions = generate_sequences(\n",
    "                                                         data_for_uid['combined_xy'].values.tolist(),\n",
    "                                                         data_for_uid['t'].values.tolist())\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train)\n",
    "test_dataset = TrajectoryDataset(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7049183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True)\n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0, batch_first=True)\n",
    "    label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    # Doing Addition here\n",
    "    return inputs_padded+positions_padded, labels_padded+label_positions_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "424aa612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Shape: torch.Size([55, 600])\n",
      "Desired output Location Shape: torch.Size([55])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "for inputs, labels in test_dataloader:\n",
    "    print(\"Location Shape:\", inputs.shape)\n",
    "    print(\"Desired output Location Shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be685e",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e39045",
   "metadata": {},
   "source": [
    "### Model Building (nn.PyTorch built-in LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "395293d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, embed_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.embed_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.embed_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Taking the output of the last sequence step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5100074",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "01df9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data related param\n",
    "BATCH_SIZE = 50\n",
    "STEP_SIZE = 600 # seq_size\n",
    "\n",
    "# Model related param\n",
    "EMBED_DIM = 256\n",
    "INPUT_DIM = 1\n",
    "LAYER_DIM = 2 \n",
    "NUM_CLASS = 40000 # 200*200 grid loc\n",
    "\n",
    "model = LSTMModel(input_dim=INPUT_DIM, embed_dim=EMBED_DIM, layer_dim=LAYER_DIM, output_dim=NUM_CLASS)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01316687985668029)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956226d5",
   "metadata": {},
   "source": [
    "```\n",
    "Best parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.01316687985668029}\n",
    "Best loss: 5.887406706809998\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ed55f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 10.638633410135904\n",
      "Epoch 1: Loss 6.900217215220134\n",
      "Epoch 2: Loss 5.385567824045817\n",
      "Epoch 3: Loss 5.324960867563884\n",
      "Epoch 4: Loss 5.251415252685547\n",
      "Epoch 5: Loss 5.121610959370931\n",
      "Epoch 6: Loss 5.1421799659729\n",
      "Epoch 7: Loss 5.152088801066081\n",
      "Epoch 8: Loss 5.067200342814128\n",
      "Epoch 9: Loss 5.041619777679443\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.float().unsqueeze(-1)\n",
    "        labels = labels.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch}: Loss {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b35ac2",
   "metadata": {},
   "source": [
    "### Hyperparameter-tuning (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc933e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59b957a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [int(i) for i in range(5,101,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1146cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Set up dataloader\n",
    "    batch_size = trial.suggest_categorical('batch_size', batch_sizes) \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model Parameters\n",
    "    NUM_CLASS = 40000\n",
    "    STEP_SIZE = 600\n",
    "    EMBED_DIM = trial.suggest_categorical('embed_dim', [64, 128, 256, 512])\n",
    "    LAYER_DIM = trial.suggest_int('layer_dim', 1, 6)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Train the model\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    \n",
    "    # Model instantiation\n",
    "    model = LSTMModel(input_dim=1,\n",
    "                      embed_dim=EMBED_DIM, \n",
    "                      layer_dim=LAYER_DIM, \n",
    "                      output_dim=NUM_CLASS)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.float().unsqueeze(-1)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    final_avg_loss = total_loss / total_samples\n",
    "    return final_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9bd9b70",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 18:45:42,611] A new study created in memory with name: no-name-c8668aba-a641-4d3e-acb8-9b0a34f35306\n",
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_16785/728437600.py:14: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
      "[I 2024-06-03 18:46:41,839] Trial 0 finished with value: 8.455938536780222 and parameters: {'batch_size': 100, 'embed_dim': 256, 'layer_dim': 5, 'learning_rate': 0.06594799113977783}. Best is trial 0 with value: 8.455938536780222.\n",
      "[I 2024-06-03 18:48:03,823] Trial 1 finished with value: 9.804386799676077 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.00011053968934568852}. Best is trial 0 with value: 8.455938536780222.\n",
      "[I 2024-06-03 18:50:53,270] Trial 2 finished with value: 7.077436123575483 and parameters: {'batch_size': 75, 'embed_dim': 512, 'layer_dim': 6, 'learning_rate': 0.007682081821027612}. Best is trial 2 with value: 7.077436123575483.\n",
      "[I 2024-06-03 18:51:24,923] Trial 3 finished with value: 5.956384319918496 and parameters: {'batch_size': 45, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.006345888623581444}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:51:45,950] Trial 4 finished with value: 6.279216980934143 and parameters: {'batch_size': 70, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.024709486116651472}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:51:53,555] Trial 5 finished with value: 10.197876971108572 and parameters: {'batch_size': 30, 'embed_dim': 64, 'layer_dim': 2, 'learning_rate': 0.0005030273127077091}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:52:38,343] Trial 6 finished with value: 5.969203148569379 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.011759643589514519}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:52:51,917] Trial 7 finished with value: 6.174463030270168 and parameters: {'batch_size': 90, 'embed_dim': 64, 'layer_dim': 5, 'learning_rate': 0.031079933847211916}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:53:57,518] Trial 8 finished with value: 6.407508523123605 and parameters: {'batch_size': 80, 'embed_dim': 256, 'layer_dim': 6, 'learning_rate': 0.0041303847208306954}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:54:46,180] Trial 9 finished with value: 10.588943658556257 and parameters: {'batch_size': 25, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 1.066437458852471e-05}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:55:27,597] Trial 10 finished with value: 7.662704830510275 and parameters: {'batch_size': 45, 'embed_dim': 512, 'layer_dim': 1, 'learning_rate': 0.0010308088718070747}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:55:35,305] Trial 11 finished with value: 7.385256683826446 and parameters: {'batch_size': 35, 'embed_dim': 128, 'layer_dim': 1, 'learning_rate': 0.0042273065542506455}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:56:03,600] Trial 12 finished with value: 8.617117711475917 and parameters: {'batch_size': 60, 'embed_dim': 128, 'layer_dim': 4, 'learning_rate': 0.0010561295989063265}. Best is trial 3 with value: 5.956384319918496.\n",
      "[I 2024-06-03 18:56:33,935] Trial 13 finished with value: 5.887406706809998 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.01316687985668029}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 18:57:04,390] Trial 14 finished with value: 8.92358089855739 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.09430822886870129}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 18:57:47,993] Trial 15 finished with value: 6.045354130438396 and parameters: {'batch_size': 15, 'embed_dim': 256, 'layer_dim': 1, 'learning_rate': 0.0023931050960067923}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 18:58:31,693] Trial 16 finished with value: 9.841769480705262 and parameters: {'batch_size': 45, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.00022402255653259143}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 18:59:42,009] Trial 17 finished with value: 5.965997648239136 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 4, 'learning_rate': 0.016172159926322933}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:00:42,130] Trial 18 finished with value: 10.281354822431291 and parameters: {'batch_size': 20, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 7.799684151741084e-05}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:01:14,546] Trial 19 finished with value: 6.922299538339887 and parameters: {'batch_size': 50, 'embed_dim': 512, 'layer_dim': 1, 'learning_rate': 0.0018152182000020036}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:01:26,059] Trial 20 finished with value: 6.09147447007043 and parameters: {'batch_size': 65, 'embed_dim': 64, 'layer_dim': 3, 'learning_rate': 0.0396439844267273}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:02:37,534] Trial 21 finished with value: 5.954916453361511 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 4, 'learning_rate': 0.01722992971861567}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:03:22,283] Trial 22 finished with value: 6.310221486432212 and parameters: {'batch_size': 95, 'embed_dim': 256, 'layer_dim': 4, 'learning_rate': 0.008568819486544494}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:04:40,579] Trial 23 finished with value: 6.1602876152311055 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 5, 'learning_rate': 0.005421018089539339}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:06:02,189] Trial 24 finished with value: 5.969635241372245 and parameters: {'batch_size': 40, 'embed_dim': 256, 'layer_dim': 4, 'learning_rate': 0.0196656987911188}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:09:56,395] Trial 25 finished with value: 7.611730168546949 and parameters: {'batch_size': 5, 'embed_dim': 256, 'layer_dim': 3, 'learning_rate': 0.04662024598838983}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:10:20,208] Trial 26 finished with value: 7.0264785085405626 and parameters: {'batch_size': 85, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.001981948983127233}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:11:13,116] Trial 27 finished with value: 6.1941790495600015 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 3, 'learning_rate': 0.012512069930207344}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:13:00,981] Trial 28 finished with value: 6.146103353159768 and parameters: {'batch_size': 55, 'embed_dim': 512, 'layer_dim': 4, 'learning_rate': 0.0036580844638650204}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:13:56,161] Trial 29 finished with value: 7.781970657621112 and parameters: {'batch_size': 100, 'embed_dim': 256, 'layer_dim': 5, 'learning_rate': 0.06614656199650501}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:14:06,070] Trial 30 finished with value: 6.347251113823482 and parameters: {'batch_size': 45, 'embed_dim': 64, 'layer_dim': 2, 'learning_rate': 0.008646120622473592}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:15:10,179] Trial 31 finished with value: 6.098486263411385 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 4, 'learning_rate': 0.01785632257804833}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:16:14,620] Trial 32 finished with value: 6.090823403426579 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 4, 'learning_rate': 0.01954605613618101}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:17:12,039] Trial 33 finished with value: 6.697683930397034 and parameters: {'batch_size': 75, 'embed_dim': 256, 'layer_dim': 5, 'learning_rate': 0.05188509223523894}. Best is trial 13 with value: 5.887406706809998.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 19:17:59,688] Trial 34 finished with value: 6.133187757219587 and parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 3, 'learning_rate': 0.005901227143346018}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:19:00,826] Trial 35 finished with value: 6.502784276008606 and parameters: {'batch_size': 70, 'embed_dim': 256, 'layer_dim': 6, 'learning_rate': 0.027215063770919395}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:20:11,016] Trial 36 finished with value: 6.170829691205706 and parameters: {'batch_size': 80, 'embed_dim': 512, 'layer_dim': 3, 'learning_rate': 0.012873985594755052}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:21:01,235] Trial 37 finished with value: 6.297863163266864 and parameters: {'batch_size': 30, 'embed_dim': 128, 'layer_dim': 4, 'learning_rate': 0.08743119231230986}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:21:12,203] Trial 38 finished with value: 6.4548179694584435 and parameters: {'batch_size': 90, 'embed_dim': 64, 'layer_dim': 3, 'learning_rate': 0.01447033672170975}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:23:20,833] Trial 39 finished with value: 7.492064673560006 and parameters: {'batch_size': 20, 'embed_dim': 256, 'layer_dim': 5, 'learning_rate': 0.0003683836623172535}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:24:50,520] Trial 40 finished with value: 7.92240891797202 and parameters: {'batch_size': 10, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.03216352698287781}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:26:31,068] Trial 41 finished with value: 5.920716605867658 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.010291229345814985}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:27:39,161] Trial 42 finished with value: 5.9336345706667215 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.009338474251830212}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:29:23,119] Trial 43 finished with value: 5.899186801910401 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.00874923267237087}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:31:03,676] Trial 44 finished with value: 5.890318424361093 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.008247042860564154}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:32:19,036] Trial 45 finished with value: 6.028782299586704 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.0034504492600436107}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:33:50,205] Trial 46 finished with value: 5.97073529788426 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.007085818180991618}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:35:15,991] Trial 47 finished with value: 6.199707405907767 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.001407669482351613}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:36:06,204] Trial 48 finished with value: 6.039394845281328 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 2, 'learning_rate': 0.002578287976090583}. Best is trial 13 with value: 5.887406706809998.\n",
      "[I 2024-06-03 19:37:31,234] Trial 49 finished with value: 6.12613343511309 and parameters: {'batch_size': 10, 'embed_dim': 128, 'layer_dim': 3, 'learning_rate': 0.009853029576276717}. Best is trial 13 with value: 5.887406706809998.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.01316687985668029}\n",
      "Best loss: 5.887406706809998\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Result\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best loss:', study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797af25e",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9a9fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Locations: tensor([19180, 17996, 17996, 17996, 22999, 17996, 17996, 22999, 17996, 17996,\n",
      "        17996, 17996, 17996, 17996, 17996, 17996, 17996, 17996, 17996, 22999,\n",
      "        17996, 17996, 22999, 22999, 17996, 17996, 17996,  2319, 17996, 18172,\n",
      "        17996, 17996, 22999, 22999, 17996, 17996, 17996, 22999, 17996, 17996,\n",
      "        22999, 17996, 17996, 19180, 17996, 17996, 22999, 17996, 17996, 22999])\n",
      "Actual Locations: tensor([28782, 18563, 34257, 17964, 16302, 15959, 17766,  3871, 19252, 16370,\n",
      "         9111, 24168, 23442, 12166, 11673, 28947, 12359, 21437, 12859, 10353,\n",
      "        26152, 40738,  8359,  2521, 27359, 22703, 26538, 21885, 40756, 16606,\n",
      "        17520, 19472, 16642,  3481, 27266, 35966, 19095, 14692, 26958, 13700,\n",
      "        22659, 25109, 14152, 29546, 25187, 15066,  8463, 17996, 24747,  3124])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.float().unsqueeze(-1)\n",
    "        logits = model(inputs)\n",
    "        probabilities = softmax(logits)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        print(f\"Predicted Locations: {predictions}\")\n",
    "        print(f\"Actual Locations: {labels}\")\n",
    "        print()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
