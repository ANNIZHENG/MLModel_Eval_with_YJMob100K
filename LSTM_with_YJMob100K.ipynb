{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cb661d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0fb9f",
   "metadata": {},
   "source": [
    "# Data-Processing \n",
    "(same process as in Transformer's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69a0f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_97331/3526287213.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_t'] = df['d']*47+df['t']\n",
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_97331/3526287213.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "yjmob1 = 'yjmob100k-dataset1.csv.gz' # dataset under normal scenes\n",
    "yjmob_df = pd.read_csv(yjmob1, compression='gzip').sort_values(by=['uid', 'd', 't'], ignore_index=True)\n",
    "\n",
    "# Retrieve all ids\n",
    "uids = yjmob_df['uid'].unique()\n",
    "\n",
    "# Just to reduce memory space\n",
    "rand_indicies = [random.randint(0, len(uids)) for _ in range(200)] # only 200 data would be used\n",
    "selected_uids = [uid for uid in uids[rand_indicies]] # selected_uids = uids[:200]\n",
    "# selected_uids = uids[:200]\n",
    "\n",
    "df = yjmob_df[yjmob_df['uid'].isin(selected_uids)] \n",
    "\n",
    "# Time\n",
    "df['combined_t'] = df['d']*47+df['t']\n",
    "\n",
    "# Location\n",
    "def spatial_token(x, y):\n",
    "    return (x-1)+(y-1)*200\n",
    "df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n",
    "\n",
    "# Sort value\n",
    "df = df.sort_values(by=['uid', 'combined_t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e5381",
   "metadata": {},
   "source": [
    "# Train-Test Split \n",
    "(~Transformer's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f59ce97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:3 split\n",
    "train_uids, test_uids = train_test_split(selected_uids, test_size=0.30, random_state=42)\n",
    "\n",
    "# Load training and testing data\n",
    "df_train = df[df['uid'].isin(train_uids)]\n",
    "df_test = df[df['uid'].isin(test_uids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069914f",
   "metadata": {},
   "source": [
    "# Batching \n",
    "(~Transformer's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2545d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STEP_SIZE = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f722893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(data, data_t):\n",
    "    return torch.tensor(data[:STEP_SIZE]),torch.tensor(data[STEP_SIZE]),\\\n",
    "                torch.tensor(data_t[:STEP_SIZE]),torch.tensor(data_t[STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1c25767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by uid\n",
    "grouped_data_train = df_train[['uid', 'combined_t', 'combined_xy']].groupby('uid')\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "grouped_data_test = df_test[['uid', 'combined_t', 'combined_xy']].groupby('uid')\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "03b87b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = grouped_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_for_uid = self.data[idx]\n",
    "        inputs, labels, positions, label_positions = generate_sequences(\n",
    "                                                         data_for_uid['combined_xy'].values.tolist(),\n",
    "                                                         data_for_uid['combined_t'].values.tolist())\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train)\n",
    "test_dataset = TrajectoryDataset(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7049183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True)\n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0, batch_first=True)\n",
    "    label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    # return inputs_padded, labels_padded, positions_padded, label_positions_padded\n",
    "    # Doing Addition here\n",
    "    return inputs_padded+positions_padded, labels_padded+label_positions_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "424aa612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Location Shape: torch.Size([50, 600])\n",
      "Desired output Location Shape: torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "for inputs, labels in test_dataloader:\n",
    "    print(\"Location Shape:\", inputs.shape)\n",
    "    print(\"Desired output Location Shape:\", labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be685e",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "395293d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, embed_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.embed_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.embed_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Taking the output of the last sequence step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "01df9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data related param\n",
    "BATCH_SIZE = 50\n",
    "STEP_SIZE = 600 # seq_size\n",
    "\n",
    "# Model related param\n",
    "EMBED_DIM = 64\n",
    "INPUT_DIM = 1\n",
    "LAYER_DIM = 1 # 1-single layer, 2 or 3-multi-layer\n",
    "NUM_CLASS = 40000 # 200*200 grid loc\n",
    "\n",
    "model = LSTMModel(input_dim=INPUT_DIM, embed_dim=EMBED_DIM, layer_dim=LAYER_DIM, output_dim=NUM_CLASS)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ed55f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 10.032061894734701\n",
      "Epoch 10: Loss 9.40379778544108\n",
      "Epoch 20: Loss 8.444165229797363\n",
      "Epoch 30: Loss 7.350537300109863\n",
      "Epoch 40: Loss 6.170390446980794\n",
      "Epoch 50: Loss 5.47230863571167\n",
      "Epoch 60: Loss 5.1818515459696455\n",
      "Epoch 70: Loss 4.999857266743978\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "epochs = 80\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = inputs.float().unsqueeze(-1)\n",
    "        labels = labels.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss {total_loss / len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9a9fe28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Locations: tensor([19180, 17996, 17996, 17996, 22999, 17996, 17996, 22999, 17996, 17996,\n",
      "        17996, 17996, 17996, 17996, 17996, 17996, 17996, 17996, 17996, 22999,\n",
      "        17996, 17996, 22999, 22999, 17996, 17996, 17996,  2319, 17996, 18172,\n",
      "        17996, 17996, 22999, 22999, 17996, 17996, 17996, 22999, 17996, 17996,\n",
      "        22999, 17996, 17996, 19180, 17996, 17996, 22999, 17996, 17996, 22999])\n",
      "Actual Locations: tensor([28782, 18563, 34257, 17964, 16302, 15959, 17766,  3871, 19252, 16370,\n",
      "         9111, 24168, 23442, 12166, 11673, 28947, 12359, 21437, 12859, 10353,\n",
      "        26152, 40738,  8359,  2521, 27359, 22703, 26538, 21885, 40756, 16606,\n",
      "        17520, 19472, 16642,  3481, 27266, 35966, 19095, 14692, 26958, 13700,\n",
      "        22659, 25109, 14152, 29546, 25187, 15066,  8463, 17996, 24747,  3124])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inference\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_dataloader:\n",
    "        inputs = inputs.float().unsqueeze(-1)\n",
    "        logits = model(inputs)\n",
    "        probabilities = softmax(logits)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "        print(f\"Predicted Locations: {predictions}\")\n",
    "        print(f\"Actual Locations: {labels}\")\n",
    "        print()\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
