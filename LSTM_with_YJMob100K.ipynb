{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb661d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c0fb9f",
   "metadata": {},
   "source": [
    "# Data-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "defecc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "yjmob1 = 'yjmob100k-dataset1.csv.gz' # dataset under normal scenes\n",
    "yjmob_df = pd.read_csv(yjmob1, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69a0f1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_29204/1116855739.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Retrieve all ids\n",
    "uids = yjmob_df['uid'].unique()\n",
    "\n",
    "# Just to reduce memory space\n",
    "rand_indicies = [random.randint(0, len(uids)) for _ in range(10000)]\n",
    "selected_uids = [uid for uid in uids[rand_indicies]]\n",
    "\n",
    "df = yjmob_df[yjmob_df['uid'].isin(selected_uids)] \n",
    "\n",
    "# Time\n",
    "# df['combined_t'] = df['d']*47+df['t']\n",
    "\n",
    "# Location\n",
    "def spatial_token(x, y):\n",
    "    return (x-1)+(y-1)*200\n",
    "\n",
    "df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07e5381",
   "metadata": {},
   "source": [
    "# Train-Test Split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f59ce97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8:2 split\n",
    "train_uids, test_uids = train_test_split(selected_uids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load training and testing data\n",
    "df_train = df[df['uid'].isin(train_uids)]\n",
    "df_test = df[df['uid'].isin(test_uids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3205bcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[['uid', 'd', 't', 'combined_xy']].to_csv('df_train.csv', index=False)\n",
    "df_test[['uid', 'd', 't', 'combined_xy']].to_csv('df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5069914f",
   "metadata": {},
   "source": [
    "# Batching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb2545d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1 # 25\n",
    "STEP_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f722893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(data, data_t):\n",
    "    return torch.tensor(data[:STEP_SIZE]),torch.tensor(data[STEP_SIZE]),\\\n",
    "                torch.tensor(data_t[:STEP_SIZE]),torch.tensor(data_t[STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c25767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by uid\n",
    "# grouped_data_train = df_train[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "# grouped_data_test = df_test[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03b87b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = grouped_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_for_uid = self.data[idx]\n",
    "        inputs, labels, positions, label_positions = generate_sequences(\n",
    "                                                         data_for_uid['combined_xy'].values.tolist(),\n",
    "                                                         data_for_uid['t'].values.tolist())\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train)\n",
    "test_dataset = TrajectoryDataset(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7049183d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True)\n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0.0, batch_first=True)\n",
    "    label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    # Doing Addition here\n",
    "    # return inputs_padded+positions_padded, labels_padded+label_positions_padded\n",
    "    return inputs_padded, labels_padded\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95be685e",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e39045",
   "metadata": {},
   "source": [
    "### Model Building (nn.PyTorch built-in LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "395293d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        self.lstm = nn.LSTM(input_dim, embed_dim, layer_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(embed_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.embed_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.embed_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Taking the output of the last sequence step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5100074",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01df9e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model related param\n",
    "EMBED_DIM = 256\n",
    "INPUT_DIM = 1\n",
    "LAYER_DIM = 1\n",
    "NUM_CLASS = 40000 # +48 # 200*200 grid loc + 48 time\n",
    "\n",
    "model = LSTMModel(input_dim=INPUT_DIM, \n",
    "                  embed_dim=EMBED_DIM, \n",
    "                  layer_dim=LAYER_DIM, \n",
    "                  output_dim=NUM_CLASS)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0013)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956226d5",
   "metadata": {},
   "source": [
    "```\n",
    "Data info: 200 data, 600 steps\n",
    "\n",
    "Best parameters: {'batch_size': 55, 'embed_dim': 256, 'layer_dim': 2, 'learning_rate': 0.01316687985668029}\n",
    "\n",
    "Best loss: 5.887406706809998\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1e136fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ed55f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.float().unsqueeze(-1)\n",
    "        labels = labels.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}: Average Loss: {total_loss/len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b35ac2",
   "metadata": {},
   "source": [
    "# Hyperparameter-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dc933e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "59b957a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [int(i) for i in range(5,101,5)]\n",
    "batch_sizes.append(1)\n",
    "batch_sizes = sorted(batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1146cbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Set up dataloader\n",
    "    batch_size = trial.suggest_categorical('batch_size', batch_sizes)  ##\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model Parameters\n",
    "    NUM_CLASS = 40000\n",
    "    STEP_SIZE = 100\n",
    "    # EMBED_DIM = trial.suggest_categorical('embed_dim', [64, 128, 256, 512])\n",
    "    EMBED_DIM = 256\n",
    "    LAYER_DIM = trial.suggest_int('layer_dim', 1, 6)  ##\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Train the model\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    \n",
    "    # Model instantiation\n",
    "    model = LSTMModel(input_dim=1,\n",
    "                      embed_dim=EMBED_DIM, \n",
    "                      layer_dim=LAYER_DIM,\n",
    "                      output_dim=NUM_CLASS)\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    total_loss = 0\n",
    "    total_samples = 0\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs = inputs.float().unsqueeze(-1)\n",
    "            labels = labels.long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    final_avg_loss = total_loss / total_samples\n",
    "    return final_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a9bd9b70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Result\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best loss:', study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797af25e",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d27ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9b191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = inputs.float().unsqueeze(-1)\n",
    "            logits = model(inputs)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            total_correct += (predictions.squeeze() == labels).sum().item()\n",
    "            total_examples += labels.numel()\n",
    "    \n",
    "    print(f\"Accuracy: {total_correct/total_examples}, Num Correct: {total_correct}, Num Sample: {total_examples}\") \n",
    "    # return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd133fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ffb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_top_k(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            inputs = inputs.float().unsqueeze(-1)\n",
    "            \n",
    "            logits = model(inputs)\n",
    "            \n",
    "            tolerated_acc = int(labels in torch.topk(logits, 5)[1])\n",
    "            total_correct += tolerated_acc\n",
    "            total_examples += labels.numel()\n",
    "    \n",
    "    print(f\"Accuracy: {total_correct/total_examples}, Num Correct: {total_correct}, Num Sample: {total_examples}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa01afe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_with_top_k(model, test_dataloader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
