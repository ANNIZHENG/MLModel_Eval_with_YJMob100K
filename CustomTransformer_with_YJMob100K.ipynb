{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0051dc88",
   "metadata": {},
   "source": [
    "**Dataset**: https://www.nature.com/articles/s41597-024-03237-9\n",
    "\n",
    "**Encoder and Decoder Reference**: https://www.youtube.com/watch?v=U0s0f995w14\n",
    "\n",
    "**user ID** is the unique identifier of the mobile phone user (type: integer)\n",
    "\n",
    "**day** is the masked date of the observation. It may take a value between 0 and 74 for both Dataset 1 and Dataset 2 (type: integer).\n",
    "\n",
    "The location pings are discretized into 500 meters × 500 meters grid cells and the timestamps are rounded up into 30-minute bins. The actual date of the observations is not available either (i.e., timeslot t of day d) to protect privacy. In the second Dataset, the 75 day period is composed of 60 days of business-as-usual and 15 days during an emergency with unusual behavior.\n",
    "\n",
    "**timeslot** is the timestamp of the observation discretized into 30 minute intervals. \n",
    "It may take a value between 0 and 47, where 0 indicates between 0AM and 0:30AM, \n",
    "and 13 would indicate the timeslot between 6:30AM and 7:00AM.\n",
    "\n",
    "**x,y** are the coordinates of the observed location mapped onto the 500 meter discretized grid cell. It may take a value between (1, 1) and (200, 200). Details are shown in Fig. 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aad1bf",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d158f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe1e83",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f611dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yjmob1 = 'yjmob100k-dataset1.csv.gz'\n",
    "# yjmob2 = 'yjmob100k-dataset2.csv.gz'\n",
    "# yjmob_df = pd.concat([pd.read_csv(yjmob1, compression='gzip'),\n",
    "#                       pd.read_csv(yjmob2, compression='gzip')]).sort_values(by=['uid','d','t'],\n",
    "#                                                                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64854975",
   "metadata": {},
   "outputs": [],
   "source": [
    "yjmob1 = 'yjmob100k-dataset1.csv.gz' # dataset under normal scenes\n",
    "yjmob_df = pd.read_csv(yjmob1, compression='gzip').sort_values(by=['uid', 'd', 't'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f862e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all ids\n",
    "\n",
    "uids = yjmob_df['uid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd5386e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to reduce memory space\n",
    "rand_indicies = [random.randint(0, len(uids)) for _ in range(200)] # only 200 data would be used\n",
    "selected_uids = [uid for uid in uids[rand_indicies]] # selected_uids = uids[:200]\n",
    "# selected_uids = uids[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "813c0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yjmob_df[yjmob_df['uid'].isin(selected_uids)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e01014",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be6c5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "# df['combined_t'] = df['d']*47+df['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearization of the 2-dimensional grid, i.e., the original x,y coordinate system\n",
    "def spatial_token(x, y):\n",
    "    # x,y are the coordinate location\n",
    "    # x determines the column order while\n",
    "    # y determines the row order\n",
    "    # (x-1) calculates the starting grid-column position\n",
    "    # (y-1)*200 calculates the start index of the grid-row\n",
    "    return (x-1)+(y-1)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "323f5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_15596/3489134120.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Location\n",
    "df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d489aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['uid', 't']) # df.sort_values(by=['uid', 'combined_t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ff03d",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49d9a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:3 split\n",
    "\n",
    "train_uids, test_uids = train_test_split(selected_uids, test_size=0.30, random_state=42)\n",
    "\n",
    "# 70 : (15:15) split\n",
    "\n",
    "# test-train split\n",
    "# train_val_uids, test_uids = train_test_split(selected_uids, test_size=0.15, random_state=42)\n",
    "\n",
    "# validation-test split\n",
    "# train_uids, val_uids = train_test_split(train_val_uids, test_size=0.176, random_state=42) # 0.176≈15/85\n",
    "\n",
    "df_train = df[df['uid'].isin(train_uids)]\n",
    "df_test = df[df['uid'].isin(test_uids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfd0c9",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "534e616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "cab04f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7983925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output input location sequence (trajectory), desired output location\n",
    "# along with the time when that user reaches a location\n",
    "\n",
    "def generate_sequences(data, data_t):\n",
    "    return torch.tensor(data[:STEP_SIZE]),torch.tensor(data[STEP_SIZE]),\\\n",
    "                torch.tensor(data_t[:STEP_SIZE]),torch.tensor(data_t[STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8e8b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by uid\n",
    "\n",
    "grouped_data_train = df_train[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "\n",
    "grouped_data_test = df_test[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9773265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x32fbe2790>]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = grouped_data_train[0]['combined_xy']\n",
    "y = grouped_data_train[0]['t']\n",
    "\n",
    "plt.plot(x[:STEP_SIZE], y[:STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8aa15331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = grouped_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_for_uid = self.data[idx]\n",
    "        inputs, labels, positions, label_positions = generate_sequences(\n",
    "                                                         data_for_uid['combined_xy'].values.tolist(),\n",
    "                                                         data_for_uid['t'].values.tolist())\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train) # train_dataset.__getitem__(0)\n",
    "test_dataset = TrajectoryDataset(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b99bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True)\n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0, batch_first=True)\n",
    "    label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    return inputs_padded, labels_padded, positions_padded, label_positions_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d96aedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc4456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "\n",
    "# data_iter = iter(train_dataloader)\n",
    "# inputs, labels, positions, label_positions = next(data_iter)\n",
    "# print(\"Shape of inputs:\", inputs.shape) # Shape: [batch_size, seq_len]\n",
    "# print(\"Shape of labels:\", labels.shape) # Shape: [batch_size]\n",
    "# print(\"Shape of positions:\", positions.shape) # Shape: [batch_size, seq_len]\n",
    "# print(\"Shape of positions:\", label_positions.shape) # Shape: [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76666eec",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c5203",
   "metadata": {},
   "source": [
    "## Input Embedding and Positional Encoding\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3e98c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f93959c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time - Positional Encoding = Time Embedding + Sequential Encoding\n",
    "# max_len is the maximum expected data length\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position.float() * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x # self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73e23499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embedding: torch.Size([25, 600, 64])\n",
      "Positional Encoding: torch.Size([25, 600, 64])\n",
      "\n",
      "Labels: torch.Size([25])\n",
      "Label_positions: torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "input_embedding_layer = nn.Embedding(40000, EMBED_DIM) # InputEmbedding(max_len, embedding_dim)\n",
    "position_embedding_layer = nn.Embedding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim\n",
    "positional_encoding = PositionalEncoding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim, dropout=0.1\n",
    "\n",
    "space_time = torch.tensor([])\n",
    "\n",
    "for inputs, labels, positions, label_positions in train_dataloader:\n",
    "    # Input Embedding\n",
    "    space = input_embedding_layer(inputs)\n",
    "    \n",
    "    # Positional Encoding\n",
    "    positions = position_embedding_layer(positions)\n",
    "    time = positional_encoding(positions)\n",
    "    \n",
    "    # Display shapes\n",
    "    print(\"Input Embedding:\", space.shape)\n",
    "    print(\"Positional Encoding:\", time.shape)\n",
    "    print()\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    print(\"Label_positions:\", label_positions.shape)\n",
    "    \n",
    "    # Addition\n",
    "    space_time = space + time\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54962ea",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e133b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f3f8a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionModule(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(MultiHeadAttentionModule, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=True):\n",
    "        # Transpose from [batch size, seq length, embed dim] to [seq length, batch size, embed dim]\n",
    "        query = query.transpose(0, 1)\n",
    "        key = key.transpose(0, 1)\n",
    "        value = value.transpose(0, 1)\n",
    "        \n",
    "        # Apply multihead attention\n",
    "        attn_output, attn_output_weights = self.multihead_attn(query, key, value, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96cc994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "\n",
    "# multihead_attention = MultiHeadAttentionModule(EMBED_DIM, NUM_HEADS)\n",
    "# attn_output = multihead_attention(space_time, space_time, space_time)\n",
    "\n",
    "# print(\"Self Attention:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f1973",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cd610aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim, num_heads, dropout_rate)\n",
    "        \n",
    "        # Normalization 1 \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion * embed_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Normalization 2\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        attn_output = self.attention(query, key, value)\n",
    "        x = self.norm1(attn_output + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(self.dropout(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caaebd7",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b60906f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, device, \n",
    "                 forward_expansion, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim).to(device)\n",
    "        self.position_embedding = nn.Embedding(time_size, embed_dim).to(device)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, embed_dim).to(device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                forward_expansion=forward_expansion,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, inputs, positions):\n",
    "        # Input Embedding\n",
    "        space = self.input_embedding(inputs)\n",
    "\n",
    "        # Positional Encoding\n",
    "        positions = self.position_embedding(positions) \n",
    "        time = self.positional_encoding(positions)\n",
    "        \n",
    "        # Addition\n",
    "        out = space + time\n",
    "\n",
    "        # Transformer Block = Multi-Head Attention + Norm + Feed Forward + Norm\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07894ebf",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5f34745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones((size, size), device=device), diagonal=1).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1d304045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, device, dropout_rate): \n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim, num_heads, dropout_rate)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, forward_expansion, dropout_rate)\n",
    "        \n",
    "    def forward(self, x, key, value, attn_mask=None):\n",
    "        attention = self.attention(x,key,value,attn_mask) \n",
    "        query = self.norm(attention + x)\n",
    "        out = self.transformer_block(query, key, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d340e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, \n",
    "                 device, forward_expansion, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim).to(device)\n",
    "        self.position_embedding = nn.Embedding(time_size, embed_dim).to(device)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, embed_dim).to(device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                forward_expansion=forward_expansion,\n",
    "                device=device,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_dim, loc_size)\n",
    "        \n",
    "    def forward(self, output, output_position, enc_out):\n",
    "        space = self.input_embedding(output.unsqueeze(1))\n",
    "        positions = self.position_embedding(output_position.unsqueeze(1))\n",
    "        time = self.positional_encoding(positions)\n",
    "        out = space + time\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, enc_out, attn_mask=None)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "59541731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "\n",
    "# input_embedding_layer = nn.Embedding(STEP_SIZE, EMBED_DIM) # InputEmbedding(max_len, embedding_dim)\n",
    "# position_embedding_layer = nn.Embedding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim\n",
    "# positional_encoding = PositionalEncoding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim\n",
    "\n",
    "# space_time = torch.tensor([])\n",
    "\n",
    "# for inputs, labels, positions, label_positions in train_dataloader:\n",
    "#     # Input Embedding\n",
    "#     space = input_embedding_layer(labels.unsqueeze(1)) # Shape: [batch_size, seq_len, embed_dim]\n",
    "    \n",
    "#     # Positional Encoding\n",
    "#     positions = position_embedding_layer(label_positions.unsqueeze(1))\n",
    "#     time = positional_encoding(positions) # Shape: [batch_size, seq_len, embed_dim]\n",
    "    \n",
    "#     # Display shapes\n",
    "#     print(\"Input Embedding:\", space.shape)\n",
    "#     print(\"Positional Encoding:\", time.shape)\n",
    "    \n",
    "#     # Addition\n",
    "#     space_time = space + time\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efac42b",
   "metadata": {},
   "source": [
    "# Transformer Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bfeb774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, \n",
    "                 device, forward_expansion, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(loc_size, time_size, embed_dim, num_layers, num_heads, device, forward_expansion, dropout_rate)\n",
    "        self.decoder = Decoder(loc_size, time_size, embed_dim, num_layers, num_heads, device, forward_expansion, dropout_rate)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src_seq, src_pos, trg_seq, trg_pos):\n",
    "        # Encode Source\n",
    "        enc_out = self.encoder(src_seq, src_pos)\n",
    "        \n",
    "        # Decode target\n",
    "        dec_out = self.decoder(trg_seq, trg_pos, enc_out)\n",
    "        \n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30859926",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "908a0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, device, learning_rate):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for inputs, labels, positions, label_positions in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        positions, label_positions = positions.to(device), label_positions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, positions, labels, label_positions)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = outputs.max(2)  # Get the index of the max log-probability\n",
    "        total_correct += (predicted.squeeze() == labels).sum().item()\n",
    "        total_samples += labels.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60305430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, device, epochs, learning_rate):\n",
    "    for epoch in range(epochs+1):\n",
    "        avg_loss, accuracy = train(model, dataloader, device, learning_rate)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2c951409",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0264a4",
   "metadata": {},
   "source": [
    "First hyperparameter results\n",
    "\n",
    "Best parameters: {'batch_size': 25, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.001317819375719244}\n",
    "\n",
    "Best loss: 3.077808467818735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b18c3c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Loss: 10.736239592234293, Accuracy: 0.0\n",
      "Epoch 1, Average Loss: 8.084031184514364, Accuracy: 0.014285714285714285\n",
      "Epoch 2, Average Loss: 5.6194775104522705, Accuracy: 0.17142857142857143\n",
      "Epoch 3, Average Loss: 3.306307792663574, Accuracy: 0.42142857142857143\n",
      "Epoch 4, Average Loss: 1.8572240471839905, Accuracy: 0.7071428571428572\n",
      "Epoch 5, Average Loss: 0.6226445982853571, Accuracy: 0.9214285714285714\n",
      "Epoch 6, Average Loss: 0.17162059992551804, Accuracy: 0.9928571428571429\n",
      "Epoch 7, Average Loss: 0.04540350909034411, Accuracy: 0.9928571428571429\n",
      "Epoch 8, Average Loss: 0.015683514919752877, Accuracy: 1.0\n",
      "Epoch 9, Average Loss: 0.005593777711813648, Accuracy: 1.0\n",
      "Epoch 10, Average Loss: 0.0014106640495204676, Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "transformer = Transformer(loc_size=40000, \n",
    "                          time_size=STEP_SIZE,\n",
    "                          embed_dim=512,\n",
    "                          num_layers=1, #?\n",
    "                          num_heads=8,\n",
    "                          device=device,\n",
    "                          forward_expansion=4,\n",
    "                          dropout_rate=0.1)\n",
    "transformer.to(device)\n",
    "train_model(transformer, train_dataloader, device, epochs=EPOCH_NUM, learning_rate=0.0013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c9e38",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b94a501",
   "metadata": {},
   "source": [
    "**Random**: Randomly select combinations of hyperparameters to test.\n",
    "\n",
    "**Bayesian**: Predict the performance of hyperparameter combinations and iteratively refines the guesses based on past results.\n",
    "\n",
    "using Hyperopt and Cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6c17d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4cd7c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [int(i) for i in range(5,101,5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eb1b5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # Set up dataloader\n",
    "    batch_size = trial.suggest_categorical('batch_size', batch_sizes) \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model Parameter\n",
    "    loc_size = 40000\n",
    "    time_size = 600\n",
    "    # embed_dim = 64\n",
    "    embed_dim = trial.suggest_categorical('embed_dim', [64, 128, 256, 512])\n",
    "    # num_layers = 4\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 6)\n",
    "    # num_heads = 8\n",
    "    num_heads = trial.suggest_categorical('num_heads', [2, 4, 8])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # forward_expansion = 128\n",
    "    forward_expansion = trial.suggest_categorical('forward_expansion', [1, 2, 4])\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    # Model\n",
    "    transformer = Transformer(loc_size=loc_size,\n",
    "                              time_size=time_size,\n",
    "                              embed_dim=embed_dim,\n",
    "                              num_layers=num_layers,\n",
    "                              num_heads=num_heads,\n",
    "                              device=device,\n",
    "                              forward_expansion=forward_expansion,\n",
    "                              dropout_rate=dropout_rate)\n",
    "    transformer.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) \n",
    "    \n",
    "    # Training loop\n",
    "    total_loss = 0\n",
    "    for epoch in range(10):\n",
    "        avg_loss, accuracy = train(transformer, train_dataloader, device, learning_rate)\n",
    "        total_loss += avg_loss\n",
    "    final_avg_loss = total_loss/10\n",
    "    \n",
    "    return final_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e85ac2cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 11:25:42,320] A new study created in memory with name: no-name-eaa2d3f1-7faf-4a43-a44a-988a30bf37ce\n",
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_15596/2293032465.py:33: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) ##\n",
      "[I 2024-06-03 11:27:42,618] Trial 0 finished with value: 6.949062967300415 and parameters: {'batch_size': 85, 'embed_dim': 512, 'num_layers': 3, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.002334897404952929}. Best is trial 0 with value: 6.949062967300415.\n",
      "[I 2024-06-03 11:29:17,305] Trial 1 finished with value: 6.925008416175842 and parameters: {'batch_size': 15, 'embed_dim': 256, 'num_layers': 6, 'num_heads': 2, 'forward_expansion': 2, 'learning_rate': 0.002034805792701858}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:30:27,247] Trial 2 finished with value: 10.346227486928305 and parameters: {'batch_size': 55, 'embed_dim': 64, 'num_layers': 5, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.0002080692858652511}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:30:33,459] Trial 3 finished with value: 9.673934178352356 and parameters: {'batch_size': 15, 'embed_dim': 64, 'num_layers': 1, 'num_heads': 2, 'forward_expansion': 1, 'learning_rate': 0.0005708808952933162}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:31:23,174] Trial 4 finished with value: 8.959108901023864 and parameters: {'batch_size': 85, 'embed_dim': 128, 'num_layers': 6, 'num_heads': 2, 'forward_expansion': 1, 'learning_rate': 0.0013268096253526687}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:31:37,841] Trial 5 finished with value: 7.604464324315389 and parameters: {'batch_size': 25, 'embed_dim': 64, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.07585329691790979}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:31:55,117] Trial 6 finished with value: 9.287213897705078 and parameters: {'batch_size': 90, 'embed_dim': 256, 'num_layers': 1, 'num_heads': 2, 'forward_expansion': 4, 'learning_rate': 0.00034245984056797697}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:32:32,872] Trial 7 finished with value: 10.622669728597007 and parameters: {'batch_size': 55, 'embed_dim': 128, 'num_layers': 4, 'num_heads': 2, 'forward_expansion': 4, 'learning_rate': 3.1951259660152056e-05}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:33:31,578] Trial 8 finished with value: 8.744151639938355 and parameters: {'batch_size': 75, 'embed_dim': 64, 'num_layers': 4, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.003317260344027487}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:35:18,226] Trial 9 finished with value: 10.041557168960571 and parameters: {'batch_size': 95, 'embed_dim': 128, 'num_layers': 5, 'num_heads': 8, 'forward_expansion': 2, 'learning_rate': 0.0002270791751553236}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:36:52,184] Trial 10 finished with value: 8.066870869909014 and parameters: {'batch_size': 10, 'embed_dim': 256, 'num_layers': 6, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.02347411465936852}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:38:32,524] Trial 11 finished with value: 7.381968629360199 and parameters: {'batch_size': 45, 'embed_dim': 512, 'num_layers': 3, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.005219021278621806}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:40:19,547] Trial 12 finished with value: 8.414248154844556 and parameters: {'batch_size': 5, 'embed_dim': 512, 'num_layers': 3, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.00928800523638166}. Best is trial 1 with value: 6.925008416175842.\n",
      "[I 2024-06-03 11:41:26,430] Trial 13 finished with value: 4.819540844857693 and parameters: {'batch_size': 85, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.0015250680146762451}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:41:59,827] Trial 14 finished with value: 10.08629150390625 and parameters: {'batch_size': 30, 'embed_dim': 256, 'num_layers': 2, 'num_heads': 2, 'forward_expansion': 2, 'learning_rate': 7.283343153394584e-05}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:43:14,874] Trial 15 finished with value: 10.451622458866666 and parameters: {'batch_size': 20, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 1.0283385671812122e-05}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:43:51,714] Trial 16 finished with value: 8.305675013860066 and parameters: {'batch_size': 50, 'embed_dim': 256, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.016348066584057602}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:45:05,886] Trial 17 finished with value: 6.985492272377013 and parameters: {'batch_size': 15, 'embed_dim': 256, 'num_layers': 5, 'num_heads': 2, 'forward_expansion': 1, 'learning_rate': 0.000948578909816251}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:47:50,502] Trial 18 finished with value: 23.911599763234456 and parameters: {'batch_size': 65, 'embed_dim': 512, 'num_layers': 4, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.06699313964185229}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:49:27,449] Trial 19 finished with value: 7.1441144943237305 and parameters: {'batch_size': 40, 'embed_dim': 256, 'num_layers': 6, 'num_heads': 2, 'forward_expansion': 2, 'learning_rate': 0.001515075775838529}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:50:32,641] Trial 20 finished with value: 7.406148211161296 and parameters: {'batch_size': 60, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.00583523145489309}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:52:32,248] Trial 21 finished with value: 7.052097511291504 and parameters: {'batch_size': 85, 'embed_dim': 512, 'num_layers': 3, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.0023416345958853124}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:54:44,684] Trial 22 finished with value: 6.066124498844147 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 3, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.0006526281405534393}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:55:54,087] Trial 23 finished with value: 5.595772564411163 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.0006140313993193643}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:57:03,782] Trial 24 finished with value: 9.350854396820068 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.00010409519086835487}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:58:13,489] Trial 25 finished with value: 5.323633354902268 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.0007056078493751214}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:58:48,237] Trial 26 finished with value: 6.6801057159900665 and parameters: {'batch_size': 35, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 0.00035372232001109905}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 11:59:56,206] Trial 27 finished with value: 9.843719959259033 and parameters: {'batch_size': 70, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 2, 'learning_rate': 6.311924195734671e-05}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 12:00:31,925] Trial 28 finished with value: 5.404775083065033 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0007986223747398082}. Best is trial 13 with value: 4.819540844857693.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-03 12:01:09,653] Trial 29 finished with value: 6.029316473007202 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.003839447945965429}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 12:01:43,290] Trial 30 finished with value: 9.589910650253296 and parameters: {'batch_size': 85, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.00015464569660014616}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 12:03:13,169] Trial 31 finished with value: 5.275576144456863 and parameters: {'batch_size': 100, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0007426159324964241}. Best is trial 13 with value: 4.819540844857693.\n",
      "[I 2024-06-03 12:04:26,598] Trial 32 finished with value: 4.58477772474289 and parameters: {'batch_size': 80, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0012730605646752314}. Best is trial 32 with value: 4.58477772474289.\n",
      "[I 2024-06-03 12:06:35,555] Trial 33 finished with value: 7.1723445653915405 and parameters: {'batch_size': 80, 'embed_dim': 512, 'num_layers': 3, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0019481806062500494}. Best is trial 32 with value: 4.58477772474289.\n",
      "[I 2024-06-03 12:07:47,090] Trial 34 finished with value: 7.225779211521148 and parameters: {'batch_size': 80, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0004003470833712051}. Best is trial 32 with value: 4.58477772474289.\n",
      "[I 2024-06-03 12:08:39,559] Trial 35 finished with value: 8.881777167320251 and parameters: {'batch_size': 80, 'embed_dim': 128, 'num_layers': 3, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.001068198328329101}. Best is trial 32 with value: 4.58477772474289.\n",
      "[I 2024-06-03 12:08:57,311] Trial 36 finished with value: 8.780528903007507 and parameters: {'batch_size': 85, 'embed_dim': 64, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0026002173871703615}. Best is trial 32 with value: 4.58477772474289.\n",
      "[I 2024-06-03 12:10:06,914] Trial 37 finished with value: 4.425840359926224 and parameters: {'batch_size': 90, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 4, 'forward_expansion': 1, 'learning_rate': 0.0012241507684812153}. Best is trial 37 with value: 4.425840359926224.\n",
      "[I 2024-06-03 12:10:47,442] Trial 38 finished with value: 4.041928515397013 and parameters: {'batch_size': 90, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.001341810954550217}. Best is trial 38 with value: 4.041928515397013.\n",
      "[I 2024-06-03 12:11:02,384] Trial 39 finished with value: 5.854645782709122 and parameters: {'batch_size': 90, 'embed_dim': 64, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.008933725099230638}. Best is trial 38 with value: 4.041928515397013.\n",
      "[I 2024-06-03 12:11:19,701] Trial 40 finished with value: 8.185666871070861 and parameters: {'batch_size': 90, 'embed_dim': 128, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.0017661623483864065}. Best is trial 38 with value: 4.041928515397013.\n",
      "[I 2024-06-03 12:12:42,940] Trial 41 finished with value: 6.862207281589508 and parameters: {'batch_size': 90, 'embed_dim': 512, 'num_layers': 2, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.000444090915872837}. Best is trial 38 with value: 4.041928515397013.\n",
      "[I 2024-06-03 12:13:26,289] Trial 42 finished with value: 3.1240926962539866 and parameters: {'batch_size': 25, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.001325930529244353}. Best is trial 42 with value: 3.1240926962539866.\n",
      "[I 2024-06-03 12:14:13,888] Trial 43 finished with value: 3.077808467818735 and parameters: {'batch_size': 25, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.001317819375719244}. Best is trial 43 with value: 3.077808467818735.\n",
      "[I 2024-06-03 12:15:01,255] Trial 44 finished with value: 6.293422238032024 and parameters: {'batch_size': 25, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.00025624351270708536}. Best is trial 43 with value: 3.077808467818735.\n",
      "[I 2024-06-03 12:15:48,814] Trial 45 finished with value: 7.3011054515838625 and parameters: {'batch_size': 25, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.0034184511368913735}. Best is trial 43 with value: 3.077808467818735.\n",
      "[I 2024-06-03 12:16:03,984] Trial 46 finished with value: 8.91899631023407 and parameters: {'batch_size': 25, 'embed_dim': 64, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.001191643542126457}. Best is trial 43 with value: 3.077808467818735.\n",
      "[I 2024-06-03 12:16:22,582] Trial 47 finished with value: 5.004035300016403 and parameters: {'batch_size': 95, 'embed_dim': 128, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.005154642989183915}. Best is trial 43 with value: 3.077808467818735.\n",
      "[I 2024-06-03 12:16:59,626] Trial 48 finished with value: 4.449930218234658 and parameters: {'batch_size': 75, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 1, 'learning_rate': 0.0012615182345643336}. Best is trial 43 with value: 3.077808467818735.\n",
      "[I 2024-06-03 12:17:42,313] Trial 49 finished with value: 4.677072602510452 and parameters: {'batch_size': 75, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.0027646960925394102}. Best is trial 43 with value: 3.077808467818735.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'batch_size': 25, 'embed_dim': 512, 'num_layers': 1, 'num_heads': 8, 'forward_expansion': 4, 'learning_rate': 0.001317819375719244}\n",
      "Best loss: 3.077808467818735\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Result\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best loss:', study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00718a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b5188be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Locations: tensor([17700, 35263, 17700, 17700, 17700, 17700, 17700, 20876, 35263, 35263,\n",
      "        17700, 17700, 17700, 20876, 35263, 35263, 17700, 17700, 17700, 17700,\n",
      "        17700, 35263, 35263, 17700, 35263, 35263, 35263, 17700, 35263, 20876,\n",
      "        17700, 14732, 17700, 35263, 17700, 35263, 17700, 17700, 35263, 17700,\n",
      "        17700, 17700, 17700, 14732, 35263, 35263, 35263, 35263, 17700, 14732])\n",
      "Actual Locations: tensor([ 6191, 10384, 22484, 17243, 16310,   873, 36849, 11195, 26317, 17982,\n",
      "        14753, 26701, 15137,  2922, 16770, 21685, 18925, 14904, 10767, 12723,\n",
      "         9578, 15534,  8711, 11308, 20267, 15728, 10581, 16547,  9179, 11126,\n",
      "        11761, 21947, 17534, 17520,  5974, 20499, 27488, 11358, 14942, 20542,\n",
      "        17754, 13928, 25929, 10989, 21350, 17340,  5370, 17338, 17352, 25314])\n",
      "\n",
      "Differences:tensor([ 11509,  24879,  -4784,    457,   1390,  16827, -19149,   9681,   8946,\n",
      "         17281,   2947,  -9001,   2563,  17954,  18493,  13578,  -1225,   2796,\n",
      "          6933,   4977,   8122,  19729,  26552,   6392,  14996,  19535,  24682,\n",
      "          1153,  26084,   9750,   5939,  -7215,    166,  17743,  11726,  14764,\n",
      "         -9788,   6342,  20321,  -2842,    -54,   3772,  -8229,   3743,  13913,\n",
      "         17923,  29893,  17925,    348, -10582])\n"
     ]
    }
   ],
   "source": [
    "# If wanting to know the exact predictions\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, positions, label_positions in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        positions, label_positions = positions.to(device), label_positions.to(device)\n",
    "        \n",
    "        logits = transformer(inputs, positions, labels, label_positions)\n",
    "        logits = logits.squeeze(1)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        print(f\"Predicted Locations: {predictions}\")\n",
    "        print(f\"Actual Locations: {labels}\")\n",
    "        print()\n",
    "        print(f\"Differences:{predictions-labels}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ebfa2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, input_sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_sequence)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4e810604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of testing where we might want to calculate the accuracy\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, positions, label_positions in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            positions = positions.to(device)\n",
    "            label_positions = label_positions.to(device)\n",
    "            \n",
    "            logits = model(inputs, positions, labels, label_positions)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            print(\"Location Difference\")\n",
    "            print(predictions.squeeze() - labels)\n",
    "            print()\n",
    "            \n",
    "            total_correct += (predictions.squeeze() == labels).sum().item()\n",
    "            total_examples += labels.numel()\n",
    "    \n",
    "    accuracy = total_correct / total_examples\n",
    "    return accuracy\n",
    "# test_accuracy = evaluate_model(transformer, test_dataloader, device)\n",
    "# print(f\"Test Accuracy: {test_accuracy}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
