{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0051dc88",
   "metadata": {},
   "source": [
    "**Dataset**: https://www.nature.com/articles/s41597-024-03237-9\n",
    "\n",
    "**Encoder and Decoder Reference**: https://www.youtube.com/watch?v=U0s0f995w14\n",
    "\n",
    "**user ID** is the unique identifier of the mobile phone user (type: integer)\n",
    "\n",
    "**day** is the masked date of the observation. It may take a value between 0 and 74 for both Dataset 1 and Dataset 2 (type: integer).\n",
    "\n",
    "The location pings are discretized into 500 meters × 500 meters grid cells and the timestamps are rounded up into 30-minute bins. The actual date of the observations is not available either (i.e., timeslot t of day d) to protect privacy. In the second Dataset, the 75 day period is composed of 60 days of business-as-usual and 15 days during an emergency with unusual behavior.\n",
    "\n",
    "**timeslot** is the timestamp of the observation discretized into 30 minute intervals. \n",
    "It may take a value between 0 and 47, where 0 indicates between 0AM and 0:30AM, \n",
    "and 13 would indicate the timeslot between 6:30AM and 7:00AM.\n",
    "\n",
    "**x,y** are the coordinates of the observed location mapped onto the 500 meter discretized grid cell. It may take a value between (1, 1) and (200, 200). Details are shown in Fig. 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aad1bf",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d158f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fe1e83",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f611dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yjmob1 = 'yjmob100k-dataset1.csv.gz'\n",
    "# yjmob2 = 'yjmob100k-dataset2.csv.gz'\n",
    "# yjmob_df = pd.concat([pd.read_csv(yjmob1, compression='gzip'),\n",
    "#                       pd.read_csv(yjmob2, compression='gzip')]).sort_values(by=['uid','d','t'],\n",
    "#                                                                             ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64854975",
   "metadata": {},
   "outputs": [],
   "source": [
    "yjmob1 = 'yjmob100k-dataset1.csv.gz' # dataset under normal scenes\n",
    "yjmob_df = pd.read_csv(yjmob1, compression='gzip').sort_values(by=['uid', 'd', 't'], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f862e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve all ids\n",
    "\n",
    "uids = yjmob_df['uid'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fd5386e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just to reduce memory space\n",
    "rand_indicies = [random.randint(0, len(uids)) for _ in range(200)] # only 200 data would be used\n",
    "selected_uids = [uid for uid in uids[rand_indicies]] # selected_uids = uids[:200]\n",
    "# selected_uids = uids[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "813c0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = yjmob_df[yjmob_df['uid'].isin(selected_uids)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e01014",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "be6c5f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "# df['combined_t'] = df['d']*47+df['t']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c4036e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linearization of the 2-dimensional grid, i.e., the original x,y coordinate system\n",
    "def spatial_token(x, y):\n",
    "    # x,y are the coordinate location\n",
    "    # x determines the column order while\n",
    "    # y determines the row order\n",
    "    # (x-1) calculates the starting grid-column position\n",
    "    # (y-1)*200 calculates the start index of the grid-row\n",
    "    return (x-1)+(y-1)*200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "323f5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hx/xp23lpqx4ndfxcvp3qj_bdgr0000gn/T/ipykernel_11976/3489134120.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)\n"
     ]
    }
   ],
   "source": [
    "# Location\n",
    "df['combined_xy'] = df.apply(lambda row: spatial_token(row['x'], row['y']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d489aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['uid', 't']) # df.sort_values(by=['uid', 'combined_t'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996ff03d",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "49d9a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7:3 split\n",
    "\n",
    "train_uids, test_uids = train_test_split(selected_uids, test_size=0.30, random_state=42)\n",
    "\n",
    "# 70 : (15:15) split\n",
    "\n",
    "# test-train split\n",
    "# train_val_uids, test_uids = train_test_split(selected_uids, test_size=0.15, random_state=42)\n",
    "\n",
    "# validation-test split\n",
    "# train_uids, val_uids = train_test_split(train_val_uids, test_size=0.176, random_state=42) # 0.176≈15/85\n",
    "\n",
    "df_train = df[df['uid'].isin(train_uids)]\n",
    "df_test = df[df['uid'].isin(test_uids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfd0c9",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "534e616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cab04f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7983925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output input location sequence (trajectory), desired output location\n",
    "# along with the time when that user reaches a location\n",
    "\n",
    "def generate_sequences(data, data_t):\n",
    "    return torch.tensor(data[:STEP_SIZE]),torch.tensor(data[STEP_SIZE]),\\\n",
    "                torch.tensor(data_t[:STEP_SIZE]),torch.tensor(data_t[STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b8e8b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by uid\n",
    "\n",
    "grouped_data_train = df_train[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "\n",
    "grouped_data_test = df_test[['uid', 't', 'combined_xy']].groupby('uid')\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9773265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x314cc3ad0>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = grouped_data_train[0]['combined_xy']\n",
    "y = grouped_data_train[0]['t']\n",
    "\n",
    "plt.plot(x[:STEP_SIZE], y[:STEP_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8aa15331",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = grouped_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data_for_uid = self.data[idx]\n",
    "        inputs, labels, positions, label_positions = generate_sequences(\n",
    "                                                         data_for_uid['combined_xy'].values.tolist(),\n",
    "                                                         data_for_uid['t'].values.tolist())\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train) # train_dataset.__getitem__(0)\n",
    "test_dataset = TrajectoryDataset(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2b99bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True)\n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0, batch_first=True)\n",
    "    label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    return inputs_padded, labels_padded, positions_padded, label_positions_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d96aedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cc4456f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "\n",
    "# data_iter = iter(train_dataloader)\n",
    "# inputs, labels, positions, label_positions = next(data_iter)\n",
    "# print(\"Shape of inputs:\", inputs.shape) # Shape: [batch_size, seq_len]\n",
    "# print(\"Shape of labels:\", labels.shape) # Shape: [batch_size]\n",
    "# print(\"Shape of positions:\", positions.shape) # Shape: [batch_size, seq_len]\n",
    "# print(\"Shape of positions:\", label_positions.shape) # Shape: [batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76666eec",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c5203",
   "metadata": {},
   "source": [
    "## Input Embedding and Positional Encoding\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c3e98c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f93959c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time - Positional Encoding = Time Embedding + Sequential Encoding\n",
    "# max_len is the maximum expected data length\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position.float() * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return x # self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "73e23499",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Embedding: torch.Size([50, 600, 64])\n",
      "Positional Encoding: torch.Size([50, 600, 64])\n",
      "\n",
      "Labels: torch.Size([50])\n",
      "Label_positions: torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "\n",
    "input_embedding_layer = nn.Embedding(40000, EMBED_DIM) # InputEmbedding(max_len, embedding_dim)\n",
    "position_embedding_layer = nn.Embedding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim\n",
    "positional_encoding = PositionalEncoding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim, dropout=0.1\n",
    "\n",
    "space_time = torch.tensor([])\n",
    "\n",
    "for inputs, labels, positions, label_positions in train_dataloader:\n",
    "    # Input Embedding\n",
    "    space = input_embedding_layer(inputs)\n",
    "    \n",
    "    # Positional Encoding\n",
    "    positions = position_embedding_layer(positions)\n",
    "    time = positional_encoding(positions)\n",
    "    \n",
    "    # Display shapes\n",
    "    print(\"Input Embedding:\", space.shape)\n",
    "    print(\"Positional Encoding:\", time.shape)\n",
    "    print()\n",
    "    print(\"Labels:\", labels.shape)\n",
    "    print(\"Label_positions:\", label_positions.shape)\n",
    "    \n",
    "    # Addition\n",
    "    space_time = space + time\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54962ea",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e133b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "f3f8a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionModule(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(MultiHeadAttentionModule, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=True):\n",
    "        # Transpose from [batch size, seq length, embed dim] to [seq length, batch size, embed dim]\n",
    "        query = query.transpose(0, 1)\n",
    "        key = key.transpose(0, 1)\n",
    "        value = value.transpose(0, 1)\n",
    "        \n",
    "        # Apply multihead attention\n",
    "        attn_output, attn_output_weights = self.multihead_attn(query, key, value, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "96cc994d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "\n",
    "# multihead_attention = MultiHeadAttentionModule(EMBED_DIM, NUM_HEADS)\n",
    "# attn_output = multihead_attention(space_time, space_time, space_time)\n",
    "\n",
    "# print(\"Self Attention:\", attn_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f1973",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cd610aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim, num_heads, dropout_rate)\n",
    "        \n",
    "        # Normalization 1 \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion * embed_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Normalization 2\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        attn_output = self.attention(query, key, value)\n",
    "        x = self.norm1(attn_output + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(self.dropout(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caaebd7",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b60906f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, device, \n",
    "                 forward_expansion, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim).to(device)\n",
    "        self.position_embedding = nn.Embedding(time_size, embed_dim).to(device)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, embed_dim).to(device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                forward_expansion=forward_expansion,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, inputs, positions):\n",
    "        # Input Embedding\n",
    "        space = self.input_embedding(inputs)\n",
    "\n",
    "        # Positional Encoding\n",
    "        positions = self.position_embedding(positions) \n",
    "        time = self.positional_encoding(positions)\n",
    "        \n",
    "        # Addition\n",
    "        out = space + time\n",
    "\n",
    "        # Transformer Block = Multi-Head Attention + Norm + Feed Forward + Norm\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07894ebf",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "83dd3998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.triu(torch.ones((size, size), device=device), diagonal=1).bool()\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1d304045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, device, dropout_rate): \n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim, num_heads, dropout_rate)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, forward_expansion, dropout_rate)\n",
    "        \n",
    "    def forward(self, x, key, value, attn_mask=None):\n",
    "        attention = self.attention(x,key,value,attn_mask) \n",
    "        query = self.norm(attention + x)\n",
    "        out = self.transformer_block(query, key, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "d340e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, \n",
    "                 device, forward_expansion, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim).to(device)\n",
    "        self.position_embedding = nn.Embedding(time_size, embed_dim).to(device)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, embed_dim).to(device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                forward_expansion=forward_expansion,\n",
    "                device=device,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_dim, loc_size)\n",
    "        \n",
    "#     def forward(self, output, output_position, enc_out):\n",
    "#         if output.dim() == 1:\n",
    "#             output = output.unsqueeze(1)  # Shape: [batch_size, 1]\n",
    "#         space = self.input_embedding(output)\n",
    "#         positions = self.position_embedding(output_position)\n",
    "#         time = self.positional_encoding(positions)\n",
    "#         out = space + time\n",
    "#         seq_len = output.size(1)  # Shape: [batch_size, seq_len]\n",
    "#         look_ahead_mask = create_look_ahead_mask(seq_len).to(self.device)\n",
    "#         look_ahead_mask = look_ahead_mask.unsqueeze(0).expand(output.size(0), -1, -1)\n",
    "#         for layer in self.layers:\n",
    "#             out = layer(out, enc_out, enc_out, attn_mask=None)\n",
    "#         out = self.fc_out(out)\n",
    "#         return out\n",
    "        \n",
    "    def forward(self, output, output_position, enc_out):\n",
    "        space = self.input_embedding(output.unsqueeze(1))\n",
    "        positions = self.position_embedding(output_position.unsqueeze(1))\n",
    "        time = self.positional_encoding(positions)\n",
    "        out = space + time\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, enc_out, attn_mask=None)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "59541731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example\n",
    "\n",
    "# input_embedding_layer = nn.Embedding(STEP_SIZE, EMBED_DIM) # InputEmbedding(max_len, embedding_dim)\n",
    "# position_embedding_layer = nn.Embedding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim\n",
    "# positional_encoding = PositionalEncoding(STEP_SIZE, EMBED_DIM) # max_len, embedding_dim\n",
    "\n",
    "# space_time = torch.tensor([])\n",
    "\n",
    "# for inputs, labels, positions, label_positions in train_dataloader:\n",
    "#     # Input Embedding\n",
    "#     space = input_embedding_layer(labels.unsqueeze(1)) # Shape: [batch_size, seq_len, embed_dim]\n",
    "    \n",
    "#     # Positional Encoding\n",
    "#     positions = position_embedding_layer(label_positions.unsqueeze(1))\n",
    "#     time = positional_encoding(positions) # Shape: [batch_size, seq_len, embed_dim]\n",
    "    \n",
    "#     # Display shapes\n",
    "#     print(\"Input Embedding:\", space.shape)\n",
    "#     print(\"Positional Encoding:\", time.shape)\n",
    "    \n",
    "#     # Addition\n",
    "#     space_time = space + time\n",
    "\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efac42b",
   "metadata": {},
   "source": [
    "# Transformer Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "bfeb774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, \n",
    "                 device, forward_expansion, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(loc_size, time_size, embed_dim, num_layers, num_heads, device, forward_expansion, dropout_rate)\n",
    "        self.decoder = Decoder(loc_size, time_size, embed_dim, num_layers, num_heads, device, forward_expansion, dropout_rate)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src_seq, src_pos, trg_seq, trg_pos):\n",
    "        # Encode Source\n",
    "        enc_out = self.encoder(src_seq, src_pos)\n",
    "        \n",
    "        # Decode target\n",
    "        dec_out = self.decoder(trg_seq, trg_pos, enc_out)\n",
    "        \n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30859926",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "908a0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, device, learning_rate):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for inputs, labels, positions, label_positions in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        positions, label_positions = positions.to(device), label_positions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, positions, labels, label_positions)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = outputs.max(2)  # Get the index of the max log-probability\n",
    "        total_correct += (predicted.squeeze() == labels).sum().item()\n",
    "        total_samples += labels.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "60305430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, device, epochs, learning_rate):\n",
    "    for epoch in range(epochs+1):\n",
    "        avg_loss, accuracy = train(model, dataloader, device, learning_rate)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss}, Accuracy: {accuracy}\")\n",
    "#         if epoch % 5 == 0:\n",
    "#             print(f\"Epoch {epoch}, Average Loss: {avg_loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "2c951409",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCH_NUM = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b18c3c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transformer = Transformer(loc_size=40000, \n",
    "                          time_size=STEP_SIZE,\n",
    "                          embed_dim=64,\n",
    "                          num_layers=4,\n",
    "                          num_heads=8,\n",
    "                          device=device,\n",
    "                          forward_expansion=128,\n",
    "                          dropout_rate=0.1,\n",
    "                          learning_rate=0.05)\n",
    "transformer.to(device)\n",
    "train_model(transformer, train_dataloader, device, epochs=EPOCH_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d73f7a",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0e691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed00718a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0b1e8230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Locations: tensor([17700, 35263, 17700, 17700, 17700, 17700, 17700, 20876, 35263, 35263,\n",
      "        17700, 17700, 17700, 20876, 35263, 35263, 17700, 17700, 17700, 17700,\n",
      "        17700, 35263, 35263, 17700, 35263, 35263, 35263, 17700, 35263, 20876,\n",
      "        17700, 14732, 17700, 35263, 17700, 35263, 17700, 17700, 35263, 17700,\n",
      "        17700, 17700, 17700, 14732, 35263, 35263, 35263, 35263, 17700, 14732])\n",
      "Actual Locations: tensor([ 6191, 10384, 22484, 17243, 16310,   873, 36849, 11195, 26317, 17982,\n",
      "        14753, 26701, 15137,  2922, 16770, 21685, 18925, 14904, 10767, 12723,\n",
      "         9578, 15534,  8711, 11308, 20267, 15728, 10581, 16547,  9179, 11126,\n",
      "        11761, 21947, 17534, 17520,  5974, 20499, 27488, 11358, 14942, 20542,\n",
      "        17754, 13928, 25929, 10989, 21350, 17340,  5370, 17338, 17352, 25314])\n",
      "\n",
      "Differences:tensor([ 11509,  24879,  -4784,    457,   1390,  16827, -19149,   9681,   8946,\n",
      "         17281,   2947,  -9001,   2563,  17954,  18493,  13578,  -1225,   2796,\n",
      "          6933,   4977,   8122,  19729,  26552,   6392,  14996,  19535,  24682,\n",
      "          1153,  26084,   9750,   5939,  -7215,    166,  17743,  11726,  14764,\n",
      "         -9788,   6342,  20321,  -2842,    -54,   3772,  -8229,   3743,  13913,\n",
      "         17923,  29893,  17925,    348, -10582])\n"
     ]
    }
   ],
   "source": [
    "# If wanting to know the exact predictions\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, positions, label_positions in test_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        positions, label_positions = positions.to(device), label_positions.to(device)\n",
    "        \n",
    "        logits = transformer(inputs, positions, labels, label_positions)\n",
    "        logits = logits.squeeze(1)\n",
    "        probabilities = F.softmax(logits, dim=1)\n",
    "        predictions = torch.argmax(probabilities, dim=1)\n",
    "\n",
    "        print(f\"Predicted Locations: {predictions}\")\n",
    "        print(f\"Actual Locations: {labels}\")\n",
    "        print()\n",
    "        print(f\"Differences:{predictions-labels}\")\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ebfa2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, input_sequence):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_sequence)\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        return probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "4e810604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of testing where we might want to calculate the accuracy\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, positions, label_positions in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            positions = positions.to(device)\n",
    "            label_positions = label_positions.to(device)\n",
    "            \n",
    "            logits = model(inputs, positions, labels, label_positions)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            \n",
    "            print(\"Location Difference\")\n",
    "            print(predictions.squeeze() - labels)\n",
    "            print()\n",
    "            \n",
    "            total_correct += (predictions.squeeze() == labels).sum().item()\n",
    "            total_examples += labels.numel()\n",
    "    \n",
    "    accuracy = total_correct / total_examples\n",
    "    return accuracy\n",
    "# test_accuracy = evaluate_model(transformer, test_dataloader, device)\n",
    "# print(f\"Test Accuracy: {test_accuracy}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cfd7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
