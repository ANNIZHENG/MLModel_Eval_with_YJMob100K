{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0051dc88",
   "metadata": {},
   "source": [
    "**Dataset**: https://www.nature.com/articles/s41597-024-03237-9\n",
    "\n",
    "**Encoder and Decoder Reference**: https://www.youtube.com/watch?v=U0s0f995w14"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aad1bf",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d158f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5cd55f",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24ce079",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('df_train.csv')[['uid', 't', 'combined_xy']]\n",
    "df_test = pd.read_csv('df_test.csv')[['uid', 't', 'combined_xy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdfd0c9",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8e8b316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group data by uid\n",
    "grouped_data_train = [group for _, group in df_train.groupby('uid')]\n",
    "grouped_data_test = [group for _, group in df_test.groupby('uid')]\n",
    "\n",
    "# Plot step data\n",
    "# plt.plot(grouped_data_train[2]['t'],grouped_data_train[2]['combined_xy'])\n",
    "# plt.plot(grouped_data_train[2]['t'][:100],grouped_data_train[2]['combined_xy'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab04f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE = 100\n",
    "WINDOW_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7983925d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_sequences(data, data_t):\n",
    "#     return torch.tensor(data[:STEP_SIZE]),torch.tensor(data[STEP_SIZE]),\\\n",
    "#                 torch.tensor(data_t[:STEP_SIZE]),torch.tensor(data_t[STEP_SIZE])\n",
    "# class TrajectoryDataset(Dataset):\n",
    "#     def __init__(self, grouped_data):\n",
    "#         self.data = grouped_data\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "#     def __getitem__(self, idx):\n",
    "#         data_for_uid = self.data[idx]\n",
    "#         inputs, labels, positions, label_positions = generate_sequences(\n",
    "#                                                          data_for_uid['combined_xy'].values.tolist(),\n",
    "#                                                          data_for_uid['t'].values.tolist())\n",
    "#         return inputs, labels, positions, label_positions\n",
    "# train_dataset = TrajectoryDataset(grouped_data_train)\n",
    "# test_dataset = TrajectoryDataset(grouped_data_test)\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, grouped_data):\n",
    "        self.data = []\n",
    "        for group in grouped_data:\n",
    "            if len(group) >= STEP_SIZE:\n",
    "                # get the first STEP_SIZE location and time data\n",
    "                xy = group['combined_xy'].values.tolist()[:STEP_SIZE]\n",
    "                t = group['t'].values.tolist()[:STEP_SIZE]\n",
    "                # slice the data into several sessions using moving window approach\n",
    "                self.data.extend([(xy[i:i+WINDOW_SIZE], t[i:i+WINDOW_SIZE])\n",
    "                                  for i in range(STEP_SIZE - WINDOW_SIZE + 1)])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        xy_window, t_window = self.data[idx]\n",
    "        inputs = torch.tensor(xy_window[:-1]) # input sequence of locations\n",
    "        labels = torch.tensor(xy_window[-1]) # desired predicted location\n",
    "        positions = torch.tensor(t_window[:-1]) # corresponding input locations' times\n",
    "        label_positions = torch.tensor(t_window[-1]) # corresponding predicted location's time\n",
    "        return inputs, labels, positions, label_positions\n",
    "\n",
    "train_dataset = TrajectoryDataset(grouped_data_train) \n",
    "test_dataset = TrajectoryDataset(grouped_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b99bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Unzip all batch\n",
    "    inputs_batch, labels_batch, positions_batch, label_positions_batch = zip(*batch)\n",
    "    \n",
    "    # Pad the sequence with less length in a batch\n",
    "    inputs_padded = torch.nn.utils.rnn.pad_sequence(inputs_batch, padding_value=0.0, batch_first=True) \n",
    "    labels_padded = torch.tensor(np.array(labels_batch))\n",
    "    positions_padded = torch.nn.utils.rnn.pad_sequence(positions_batch, padding_value=0, batch_first=True) \n",
    "    label_positions_padded = torch.tensor(np.array(label_positions_batch))\n",
    "    \n",
    "    return inputs_padded, labels_padded, positions_padded, label_positions_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "534e616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = (len(train_dataset) // len(grouped_data_train)) * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d96aedf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76666eec",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c5203",
   "metadata": {},
   "source": [
    "## Input Embedding and Positional Encoding\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html#load-and-batch-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f93959c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time - Positional Encoding = Time Embedding + Sequential Encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \n",
    "    def __init__(self, max_len, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2) * (-np.log(10000.0) / embedding_dim))\n",
    "        pe = torch.zeros(max_len, 1, embedding_dim)\n",
    "        pe[:, 0, 0::2] = torch.sin(position.float() * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position.float() * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)].squeeze(1)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54962ea",
   "metadata": {},
   "source": [
    "## Self-Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e133b7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3f8a987",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionModule(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout_rate):\n",
    "        super(MultiHeadAttentionModule, self).__init__()\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None, need_weights=True):\n",
    "        # Transpose from [batch size, seq length, embed dim] to [seq length, batch size, embed dim]\n",
    "        query = query.transpose(0, 1)\n",
    "        key = key.transpose(0, 1)\n",
    "        value = value.transpose(0, 1)\n",
    "        \n",
    "        # Apply multihead attention\n",
    "        attn_output, attn_output_weights = self.multihead_attn(query, key, value, attn_mask=attn_mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        return attn_output.transpose(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479f1973",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd610aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout_rate):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim, num_heads, dropout_rate)\n",
    "        \n",
    "        # Normalization 1 \n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Feed-Forward\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, forward_expansion * embed_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
    "        )\n",
    "        \n",
    "        # Normalization 2\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        attn_output = self.attention(query, key, value)\n",
    "        x = self.norm1(attn_output + query)\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.norm2(self.dropout(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5caaebd7",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b60906f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, device, \n",
    "                 forward_expansion, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim).to(device)\n",
    "        self.position_embedding = nn.Embedding(time_size, embed_dim).to(device)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, embed_dim).to(device)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                forward_expansion=forward_expansion,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, inputs, positions):\n",
    "        # Input Embedding\n",
    "        space = self.input_embedding(inputs)\n",
    "\n",
    "        # Positional Encoding\n",
    "        positions = self.position_embedding(positions) \n",
    "        time = self.positional_encoding(positions)\n",
    "        \n",
    "        # Addition\n",
    "        out = space + time\n",
    "\n",
    "        # Transformer Block = Multi-Head Attention + Norm + Feed Forward + Norm\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07894ebf",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d304045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, forward_expansion, device, dropout_rate): \n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttentionModule(embed_dim, num_heads, dropout_rate)\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.transformer_block = TransformerBlock(embed_dim, num_heads, forward_expansion, dropout_rate)\n",
    "        \n",
    "    def forward(self, x, key, value, attn_mask=None):\n",
    "        attention = self.attention(x,key,value,attn_mask) \n",
    "        query = self.norm(attention + x)\n",
    "        out = self.transformer_block(query, key, value)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d340e4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, \n",
    "                 device, forward_expansion, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.device = device\n",
    "        self.input_embedding = nn.Embedding(loc_size, embed_dim).to(device)\n",
    "        self.position_embedding = nn.Embedding(time_size, embed_dim).to(device)\n",
    "        self.positional_encoding = PositionalEncoding(time_size, embed_dim).to(device)\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderBlock(\n",
    "                embed_dim,\n",
    "                num_heads,\n",
    "                forward_expansion=forward_expansion,\n",
    "                device=device,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.fc_out = nn.Linear(embed_dim, loc_size)\n",
    "        \n",
    "    def forward(self, output, output_position, enc_out):\n",
    "        space = self.input_embedding(output.unsqueeze(1))\n",
    "        positions = self.position_embedding(output_position.unsqueeze(1))\n",
    "        time = self.positional_encoding(positions)\n",
    "        out = space + time\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, enc_out, enc_out, attn_mask=None)\n",
    "        out = self.fc_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efac42b",
   "metadata": {},
   "source": [
    "# Transformer Training Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfeb774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, loc_size, time_size, embed_dim, num_layers, num_heads, \n",
    "                 device, forward_expansion, dropout_rate):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(loc_size, time_size, embed_dim, \n",
    "                               num_layers, num_heads, device, forward_expansion, dropout_rate)\n",
    "        self.decoder = Decoder(loc_size, time_size, embed_dim, \n",
    "                               num_layers, num_heads, device, forward_expansion, dropout_rate)\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src_seq, src_pos, trg_seq, trg_pos):\n",
    "        # Encode Source\n",
    "        enc_out = self.encoder(src_seq, src_pos)\n",
    "        # Decode target\n",
    "        dec_out = self.decoder(trg_seq, trg_pos, enc_out)\n",
    "        return dec_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30859926",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "908a0aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, device, learning_rate):\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for inputs, labels, positions, label_positions in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        positions, label_positions = positions.to(device), label_positions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs, positions, labels, label_positions)\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), labels.view(-1))\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = outputs.max(2)  # Get the index of the max log-probability\n",
    "        total_correct += (predicted.squeeze() == labels).sum().item()\n",
    "        total_samples += labels.numel()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60305430",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, device, epochs, learning_rate):\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss, accuracy = train(model, dataloader, device, learning_rate)\n",
    "        print(f\"Epoch {epoch}, Average Loss: {avg_loss}, Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b18c3c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCH_NUM = 2\n",
    "transformer = Transformer(loc_size=40000, \n",
    "                          time_size=48,\n",
    "                          embed_dim=256,\n",
    "                          num_layers=1,\n",
    "                          num_heads=8,\n",
    "                          device=device,\n",
    "                          forward_expansion=4,\n",
    "                          dropout_rate=0.1)\n",
    "transformer.to(device)\n",
    "train_model(transformer, train_dataloader, device, epochs=EPOCH_NUM, learning_rate=0.0013)\n",
    "\n",
    "\"\"\"\n",
    "Epoch 0, Average Loss: 1.5751103802556257, Accuracy: 0.863751992964745\n",
    "Epoch 1, Average Loss: 0.08293261576167783, Accuracy: 0.993369415230183\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00718a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ba173ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4e810604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_top_k(model, dataloader, device, k):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_examples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, positions, label_positions in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            positions = positions.to(device)\n",
    "            label_positions = label_positions.to(device)\n",
    "            \n",
    "            logits = model(inputs, positions, labels, label_positions)\n",
    "            \n",
    "            if (BATCH_SIZE > 1):\n",
    "                for logit_i in range(len(logits)):\n",
    "                    tolerated_acc = int(labels[logit_i] in torch.topk(logits[logit_i], k)[1])\n",
    "                    total_correct += tolerated_acc\n",
    "            else:\n",
    "                tolerated_acc = int(labels in torch.topk(logits, k)[1])\n",
    "                total_correct += tolerated_acc\n",
    "            \n",
    "            total_examples += labels.numel()\n",
    "    \n",
    "    print(f\"Batch Size: {BATCH_SIZE} Top {k} Accuracy: {total_correct/total_examples}, Num Correct: {total_correct}, Num Sample: {total_examples}\")\n",
    "    return total_correct/total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81c34d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_with_top_k(transformer, test_dataloader, device, 5)\n",
    "\n",
    "\"\"\"\n",
    "Batch Size: 910 Top 5 Accuracy: 0.9818553539483772, Num Correct: 176732, Num Sample: 179998\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50c9e38",
   "metadata": {},
   "source": [
    "# Hyperparameter-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17d024",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd7c56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = [int(i) for i in range(5,101,5)]\n",
    "batch_sizes.append(1)\n",
    "batch_sizes = sorted(batch_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1b5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \n",
    "    # Set up dataloader\n",
    "    batch_size = trial.suggest_categorical('batch_size', batch_sizes)  ##\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    # Model Parameter\n",
    "    loc_size = 40000\n",
    "    time_size = 600\n",
    "    embed_dim = 256\n",
    "    # embed_dim = trial.suggest_categorical('embed_dim', [64, 128, 256, 512])\n",
    "    # num_layers = 1\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 6)  ##\n",
    "    num_heads = 8\n",
    "    # num_heads = trial.suggest_categorical('num_heads', [2, 4, 8])\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    forward_expansion = 4\n",
    "    # forward_expansion = trial.suggest_categorical('forward_expansion', [1, 2, 4])\n",
    "    dropout_rate = 0.1\n",
    "    \n",
    "    # Model\n",
    "    transformer = Transformer(loc_size=loc_size,\n",
    "                              time_size=time_size,\n",
    "                              embed_dim=embed_dim,\n",
    "                              num_layers=num_layers,\n",
    "                              num_heads=num_heads,\n",
    "                              device=device,\n",
    "                              forward_expansion=forward_expansion,\n",
    "                              dropout_rate=dropout_rate)\n",
    "    transformer.to(device)\n",
    "    \n",
    "    # Train the model\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) \n",
    "    \n",
    "    # Training loop\n",
    "    total_loss = 0\n",
    "    for epoch in range(5):\n",
    "        avg_loss, accuracy = train(transformer, train_dataloader, device, learning_rate)\n",
    "        total_loss += avg_loss\n",
    "    final_avg_loss = total_loss/10\n",
    "    \n",
    "    return final_avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85ac2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study()\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Result\n",
    "print('Best parameters:', study.best_params)\n",
    "print('Best loss:', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
